---
permalink: /
title: "Nick (Hengrui) Jia"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm a PhD student at the [CleverHans Lab](https://cleverhans-lab.github.io/) at 
the [Vector Insititute](https://vectorinstitute.ai/) and 
[University of Toronto](https://www.utoronto.ca/), advised by 
[Prof. Nicolas Papernot](https://www.papernot.fr/). 

My research interests are at the intersection of security and machine learning, or trustworthy machine learning. 
In particular, I wish to answer the following question: what risks may come along with the benefits brought by 
machine learning, what/who is responsible for such risks, and how can we mitigate them? If you would like to learn more
 about my research, I recommend reading my publications listed below, or the corresponding blog posts such as 
 [proof-of-learning](http://www.cleverhans.io/2021/07/30/pol.html), or 
 [machine unlearning](http://www.cleverhans.io/2020/07/20/unlearning.html).

Publications
======
<a href="https://arxiv.org/abs/2307.00310" target="_blank">Backdoor Detection through Replicated Execution of Outsourced Training</a>.
              <i><b>Hengrui Jia</b>, Sierra Wyllie, Akram Bin Sediq, Ahmed Ibrahim, Nicolas Papernot.</i>
              Proceedings of 3rd IEEE Conference on Secure and Trustworthy Machine Learning.

<a href="https://arxiv.org/abs/2307.00310" target="_blank">LLM Dataset Inference: Did you train on my dataset?</a>.
              <i>Pratyush Maini, <b>Hengrui Jia</b>, Nicolas Papernot, Adam Dziedzic.</i>
              Proceedings of the 38th Annual Conference on Neural Information Processing Systems.

<a href="https://arxiv.org/abs/2307.00310" target="_blank">Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD</a>.
              <i>Anvith Thudi, <b>Hengrui Jia</b>, Casey Meehan, Ilia Shumailov, Nicolas Papernot.</i>
              Proceedings of the 33rd USENIX Security Symposium.
              
<a href="https://arxiv.org/abs/2208.03567" target="_blank">Proof-of-Learning is Currently More Broken Than You Think</a>.
              <i>Congyu Fang, <b>Hengrui Jia</b>, Anvith Thudi, Mohammad Yaghini, Christopher A. Choquette-Choo, Natalie Dullerud, Varun Chandrasekaran, Nicolas Papernot.</i>
              Proceedings of the 8th IEEE European Symposium on Security and Privacy.

<a href="https://arxiv.org/abs/2110.11891" target="_blank">On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning</a>.
              <i>Anvith Thudi, <b>Hengrui Jia</b>, Ilia Shumailov, Nicolas Papernot.</i>
              Proceedings of the 31st USENIX Security Symposium.
             
<a href="https://openreview.net/forum?id=OUz_9TiTv9j" target="_blank">A Zest of LIME: Towards Architecture-Independent Model Distances</a>.
              <i><b>Hengrui Jia</b>, Hongyu Chen, Jonas Guan, Ali Shahin Shamsabadi, Nicolas Papernot.</i>
              Proceedings of the 10th International Conference on Learning Representations.
              
<a href="https://arxiv.org/abs/2109.10870" target="_blank">SoK: Machine Learning Governance</a>.
              <i>Varun Chandrasekaran, <b>Hengrui Jia</b>, Anvith Thudi, Adelin Travers, Mohammad Yaghini, Nicolas Papernot.</i>
              Arxiv preprint.

<a href="https://arxiv.org/abs/2103.05633" target="_blank">Proof-of-Learning: Definitions and Practice</a>.
              <i><b>Hengrui Jia</b>, Mohammad Yaghini, Christopher A. Choquette-Choo, Natalie Dullerud, Anvith Thudi,
                Varun Chandrasekaran, Nicolas Papernot.</i>
              Proceedings of the 42nd IEEE Symposium on Security and Privacy, San Francisco, CA. 

<a href="https://arxiv.org/abs/2002.12200" target="_blank">Entangled Watermarks as a Defense against Model Extraction</a>.
              <i><b>Hengrui Jia</b>, Christopher A. Choquette-Choo, Varun Chandrasekaran, Nicolas Papernot.</i>
              Proceedings of the 30th USENIX Security Symposium.
              
<a href="https://arxiv.org/abs/1912.03817" target="_blank">Machine Unlearning</a>.
              <i>Lucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, <b>Hengrui Jia</b>, Adelin Travers,
                Baiwu Zhang, David Lie, Nicolas Papernot.</i>
              Proceedings of the 42nd IEEE Symposium on Security and Privacy, San Francisco, CA.

Awards
======
**Ontario Graduate Scholarship**, Province of Ontario and University of Toronto (2024-2025)

**Ontario Graduate Scholarship**, Province of Ontario and University of Toronto (2023-2024)

**Mary H. Beatty Fellowship**, University of Toronto (2022-2023)

**Vector Scholarship in Artificial Intelligence**, Vector Institute (2020-2021)

**Dean's List**, University of Toronto (2016-2020)


Invited Talks
======
Ownership Resolution in ML, Purdue University (2024)

Ownership Resolution in ML, Northwestern University (2024)

Ownership Resolution in ML, University of Wisconsinâ€“Madison (2024)

Ownership of ML Models, Mila - Quebec AI Institute (2024)

Entangled Watermarks as a Defense against Model Extraction, DeepMind (2023)

A Zest of LIME: Towards Architecture-Independent Model Distances, Workshop on Algorithmic Audits of Algorithms (2023)

Entangled Watermarks as a Defense against Model Extraction, Intel (2022)

Machine Unlearning, Vector Institute (2021)
